# MySQL中的索引

## 概述

MySQL官方对索引的定义为:帮助MySQL高效获取数据的数据结构。

索引的本质是数据结构，相当于“排好序的快速查找数据结构”

##  索引的作用（优点）

1. 索引大大减小了服务器需要扫描的数据量，从而大大加快数据的检索速度，这也是创建索引的最主要原因；

2. 通过创建唯一性索引，保证了数据库的表中每一行数据的唯一性；

3. 索引可以帮助服务器避免排序和创建临时表；

4. 在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间。

## 索引的缺点

1. 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 INSERT、UPDATE和DELETE；因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。

2. 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件会膨胀很快。

## 索引的分类

索引主要有5类，包括普通索引、唯一索引、主键索引、联合索引、全文索引

### 普通索引

是最基本的索引，它没有任何限制

### 唯一索引

与普通索引类似，不同的是唯一索引索引列的值必须是唯一的，但允许有空值。如果是组合索引，则列值的组合必须唯一

### 主键索引

它是一种特殊的唯一索引，用于唯一标识数据库表中的某一条记录，不允许有空值，一般用primary key来约束

### 联合索引

又称为复合索引，它是在多个字段上建立的索引，能够加速复合查询条件的检索

遵循**最左匹配原则**，只有最左边的字段才能使用到联合索引（查询条件中必须包含最左边的列`A`）

![联合索引最左匹配原则](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231117143430803.png)

### 全文索引

老版本MySQL自带的全文索引只能用于数据库引擎为MyISAM的数据表，新版本MySQL 5.6的InnoDB支持全文索引。默认MySQL不支持中文全文检索，可以通过扩展MySQL，添加中文全文检索或为中文内容表提供一个对应的英文索引表的方式来支持中文。

## 索引的底层数据结构

索引是在MySQL的存储引擎层中实现的，而不是在服务层实现的。

#### 存储引擎

- MyISAM存储引擎

  - 访问快,**不支持事务和外键**。表结构保存在.frm文件中，表数据保存在.MYD文件中，索引保存在.MYI文件中。

- InnoDB存储引擎(MySQL5.5版本后默认的存储引擎)

  - 支持事务 ,占用磁盘空间大 ,支持并发控制。  表结构保存在.frm文件中，如果是共享表空间，数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。如果是多表空间存储，每个表的数据和索引单独保存在 .ibd 中。

- MEMORY存储引擎 
  - 内存存储 , 速度快 ,不安全 ,适合小量快速访问的数据。表结构保存在.frm中。



MySQL目前有4中索引：B+Tree索引、Hash索引、空间索引（R-Tree索引）、全文索引（S-Full-text)。

### B+Tree索引

是最常见的索引类型，大部分索引都支持B+Tree索引

### Hash索引

只有Memory引擎支持，使用场景简单

###  空间索引（R-Tree索引）

空间索引是MyISAM的一个特殊索引类型，用于处理地理空间信息，可以对地理空间数据进行高效的查询。

### 全文索引（S-Full-text)

也是MyISAM的一个特殊索引类型，主要用于全文索引，InnoDB从MySQL5.6版本开始支持全文索引。

## B+Tree

### B+Tree是BTree的变种

1. B+Tree的叶子节点保存所有的key信息，依key的大小顺序排序；
2. B+Tree叶子节点元素维护了一个单向链表，所有非叶子节点都可以看作是key的索引部分，由于B+Tree只有叶子节点保存key信息，查询任何key都要从root走到叶子，所以B+tree的查询效率更稳定。

### MySQL中的B+Tree

MySQL索引数据结构对B+Tree进行了优化，增加了一个指向相邻叶子节点的链表指针，形成了带有顺序指针的 B+Tree, 提高区间访问的性能。

## 回表查询

当执行查询时，如果所有需要的数据都可以直接从索引中获取，那么MySQL就无需再访问主表，这可以大大提高查询效率。

然而，如果查询需要的某些数据不在覆盖索引中，那么MySQL就需要再次访问主表来获取这些数据，这就是所谓的"回表查询"。由于需要额外访问主表，回表查询通常会比只使用覆盖索引的查询慢。

覆盖索引： 覆盖索引是select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖。 

## 索引下推

索引下推（Index Condition Pushdown，简称ICP）是MySQL 5.6版本引入的一种查询优化策略。

在MySQL中，当我们使用联合索引进行查询时，通常的做法是先通过索引找到符合条件的数据，然后再去主表中获取完整的数据。这个过程中，MySQL服务器需要先从存储引擎获取数据，然后再判断这些数据是否符合查询条件1。

然而，从MySQL 5.6版本开始，引入了一种新的优化策略，叫做索引下推。在使用索引下推的情况下，MySQL服务器会将查询条件直接传递给存储引擎，然后存储引擎在索引中就可以判断哪些数据是符合条件的1。

这样做的好处是，存储引擎只需要返回符合条件的数据，这样就减少了存储引擎查询基础表的次数，也减少了MySQL服务器从存储引擎接收数据的次数，从而提高了查询效率。

# 在项目中如何对mysql做性能调优?

系统上线后,发现运行非常慢,是由一条sql导致的,请问如何优化?

系统上线以后,发现运行非常慢,请问如何排查?

### 性能调优思路

![mysql性能调优思路](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231121143150885.png)



## 定位执行效率慢的 sql 语句

- 慢查询日志(常用的工具) 

  慢查询日志记录了所有执行时间超过参数 long_query_time 的sql 语句的日志 , long_query_time 默认为 10 秒(可以通过配置文件设置), 日志保存在 /var/lib/mysql/目录下, 有个 slow_query.log 文件, 

![满查询日志语句](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231121142414016.png)

- 命令:`show status like Com_` ,通过这条命令, 我们可以知道当前数据库是以查询为主 还是更新为主. 如果是查询为主, 就重点查询; 如果增删改多就重点优化写入操作.

  ![查询操作次数](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231121142016478.png)

- show profile 分析 SQL,可以查看所有 sql 语句的执行效率(所用时间). 前提是这个命令需要 被打开, 严格的说也就是打开这个命令后执行的所有 sql 语句, 它都能记录下执行时间, 并展示出来. 可以通过这个命令分析哪些 sql 语句执行效率低. 耗时长, 就更有针对性的优化 这条 sql. 


## explain + sql语句查询sql执行过程

通过执行计划，我们能得到以下信息：

- A：哪些步骤花费的成本比较高
- B：哪些步骤产生的数据量多，数据量的多少用线条的粗细表示，很直观
- C：这条sql语句是否走索引

## 优化索引

### 索引设计原则

索引的设计需要遵循一些已有的原则, 这样便于提升索引的使用效率, 更高效的使用索引. 

* 对查询频次较高, 且数据量比较大的表, 建立索引. 

* 索引字段的选择, 最佳候选列应当从 where 子句的条件中提取, 如果 where 子句中的组合比较多, 那么应当挑选最常用, 过滤效果最好的列的组合. 

* 使用唯一索引, 区分度越高, 使用索引的效率越高. 

* 索引并非越多越好, 如果该表赠,删,改操作较多, 慎重选择建立索引, 过多索引会降低表维 护效率. 

* 使用短索引, 提高索引访问时的 I/O 效率, 因此也相应提升了 Mysql 查询效率. 

* 如果 where 后有多个条件经常被用到, 建议建立符合索引, 复合索引需要遵循最左前缀法则, N 个列组合而成的复合索引, 相当于创建了 N 个索引. 

  复合索引命名规则 index\_表名\_列名 1\_列名 2\_列名 3 

  比如:create index idx_seller_name_sta_addr on tb_seller(name, status, address )

###  避免索引失效 

* 如果在查询的时候, 使用了复合索引, 要遵循最左前缀法则, 也就是查询从索引的最左列开始, 并且不能跳过索引中的列. 
* 尽量不要在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描 .
* 应尽量避免在 where 子句中使用 != 或 < > 操作符，否则将引擎放弃使用索引而进行全表扫描。 
* 不做列运算where age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函 数.计算表达式等, 都会是索引失效. 
* 查询 like，如果是 `%aaa` 也会造成索引失效,导致查询过程不走索引. 
* 应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描 

##   Sql 语句调优 

* 根据业务场景建立复合索引只查询业务需要的字段，如果这些字段被索引覆盖，将极大的提高查询效率. 
* 多表连接的字段上需要建立索引，这样可以极大提高表连接的效率. 
* where 条件字段上需要建立索引, 但 Where 条件上不要使用运算函数，以免索引失效. 
* 排序字段上, 因为排序效率低, 添加索引能提高查询效率. 
* 优化 insert 语句: 批量列插入数据要比单个列插入数据效率高. 
* 优化 order by 语句: 在使用 order by 语句时, 不要使用 select \*, select 后面要查有 索引的列, 如果一条 sql 语句中对多个列进行排序, 在业务允许情况下, 尽量同时用升 序或同时用降序. 
* 优化 group by 语句: 在我们对某一个字段进行分组的时候, Mysql 默认就进行了排序, 但是排序并不是我们业务所需的, 额外的排序会降低效率. 所以在用的时候可以禁止 排序, 使用 order by null 禁用. select age, count(*) from emp group by age order by null 
* 尽量避免子查询, 可以将子查询优化为 join 多表连接查询. 



 ## 合理的数据库设计 

~~~properties
//​	根据**数据库三范**式来进行表结构的设计。设计表结构时，就需要考虑如何设计才能更有效的查询, 遵循数据库三范式： 

//1. 第一范式：数据表中每个字段都必须是不可拆分的最小单元，也就是确保每一列的原子性； 
//2. 第二范式：满足一范式后，表中每一列必须有唯一性，都必须依赖于主键； 
//3. 第三范式：满足二范式后，表中的每一列只与主键直接相关而不是间接相关(外键 也是直接相关)，字段没有冗余。 
~~~



 有时候可以根据场景合理地反规范化:

1. 保留冗余字段。当两个或多个表在查询中经常需要连接时，可以在其中一个表上增加 若干冗余的字段，以 避免表之间的连接过于频繁，一般在冗余列的数据不经常变动的情况下使用。

2. 增加派生列。派生列是由表中的其它多个列的计算所得，增加派生列可以减少统计 运算，在数据汇总时可以大大缩短运算时间, 前提是这个列经常被用到, 这也就是反第三范式。 

3. 分割表 

   数据表拆分：主要就是垂直拆分和水平拆分。 水平切分:将记录散列到不同的表中，各表的结构完全相同，每次从分表中查询, 提高 效率。 垂直切分:将表中大字段单独拆分到另外一张表, 形成一对一的关系。 

4. 字段设计

   1. 表的字段尽可能用 NOT NULL 
   2. 字段长度固定的表查询会更快 
   3. 把数据库的大表按时间或一些标志分成小表



## 其他优化

- 修改mysql服务器的配置, 比如：增大缓存innodb_buffer_pool_size

- 增加硬件cpu 、内存

  

# 你的项目中自定义过AOP吗?什么场景?如何实现的?

## 需求是什么

​	当时的有一个业务需求是记录操作人员,操作时间,业务执行时长,并将这些信息保存到数据库中



​	为了记录操作所以使用环绕通知



## 定义的过程



当时做了一个管理系统,在增删改员工的时候想把操作执行的相关信息记录到数据库，比如：操作人id,操作时间,操作类名,操作方法名，但在每一个方法上都单独记录的话，就太繁琐了,所以我就使用了aop。
实现:
1.要先写一个切面类,在类上加上了@Aspect跟@Compoent注解，这样这个切面类就可以被ioc容器管理
2.因为要获取方法执行的时长,所以我选用了@Around环绕通知
3.因为要匹配业务接口当中所有的增删改的方法，所以选择了注解的形式,所以我们使用了annotation表达式
4.最后往数据库插入的时候我们又设置一个异步处理，,开启了一 个新的线程， 这就避免了如果我们往数据库存数据的时候如果出现了异常那么不会影响主程序的正常运行.



![自定义AOP（步骤2）](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231123143642174.png)

# 有没有自定义过starter

## 需求是什么

​	使用七牛云作为文件服务器,上传文件到七牛云服务器的

## 定义的过程


1. 先是按照公司提供的上传云端的方式需求写了一个工具类,
   将工具类和所需的依赖放入一个autoconfigure的模块,
2. 为了方便主程序配置参数能够直接用，写了一个参数配置类可以直接读取yaml配置参数
3. 又定义了个Spring自动装配的类(@Configuration),设置好bean对象
4. 模仿这Spring的自动装配的设置，在autoconfigure中的resouce中创建配置读取的文件夹和一 个Spring.factories 文件,将含有bean的文件的全包名配置到文件中
5. 将这个autoconfigure配置好，再创建一个starter模块, 直接pom中引入之前创建的autoconfigure模块
6. 项目可以直接引入我的starter依赖，就能用七牛云的服务

![image-20231124143155537](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231124143155537.png)

# 讲一下springboot自动装配的原理



 SpringBoot 是如何完成自动配置的呢? 

1. SpringBoot 自动配置的注解是 @EnableAutoConfiguration. 
2. 我们用的时候是在启动类上加@SpringBootApplication,这个注解是复合注解,内部包含 @EnableAutoConfiguration 
3. @EnableAutoConfiguration 内部有一个@Import, 这个注解才是完成自动配置的关键. 
4. @Import 导入一个类(AutoConfigurationImportSelector),这个类内部提供了一个方 法(selectImports). 这个方法会扫描导入的所有 jar 包下的 spring.factories 文件. 解析文 件中自动配置类 key=value, 将列表中的类创建,并放到 Spring 容器中 

![SpringBoot自动装配的原理](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231213142459422.png)

1. 自动装配就是自动地把其他组件中的Bean装载到IOC容器中，不需要我们再去配置文件中添加大量的配置

2. SpringBoot通过@EnableAutoConfiguration注解开启自动配置,这个注解内部有一个@lmport注解

3. @Import导入一个类(AutoConfigurationlmportSelector)，这个类内部提供了一个方法(selectlmports).这个方法会扫描导入的所有jar包下的META-INF/spring.factories文件。解析文件中自动配置类,实例化类中定义的bean,并放到Spring容器中
    4.总之一个@SpringBootApplication 注解就搞定了所有事,它封装了核心的
    @SpringBootConfiguration+@EnableAutoConfiguration+@ComponentScan这三个注解

  

2. @SpringBootConfiguration:表示该类为主配置类，声明定义Bean

2. @ComponentScan:扫描主配置类包的所有包下的类
3. @EnableAutoConfiguration:开启自动装配类



# 讲一下你们系统的登录是如何来做的?

通过查询数据库验证用户身份。因为我们用使用MD5加密.
比对密码进行密码匹配的验证。若密码不匹配或用户不存在，则抛异常。
在成功验证用户身份后，通过JWT令牌进行安全认证。借助JwtService生成令牌，并将其返回给前端，
前端保存token到浏览器的local storage中,
后续的请求需要在拦截其中上传token对token进行校验
为了在整个应用程序中方便地传递当前用户的ID，同时也是为了保证线程的安全使用ThreadLocal将用户ID保存到当前线程中。
在登录成功后可以轻松地通过ThreadLocal获取当前用户ID。


# ThreadLocal的实现原理

​	ThreadLocal：为共享变量在每个线程中创建一个副本，每个线程都可以访问自己 内部的副本变量。通过 threadlocal 保证线程的安全性。 

​	其实在 ThreadLocal 类中有一个静态内部类 ThreadLocalMap(其类似于 Map)， 用键值对的形式存储每一个线程的变量副本，ThreadLocalMap 中元素的 key 为当前 ThreadLocal 对象，而 value 对应线程的变量副本。 

​	ThreadLocal 本身并不存储值，它只是作为一个 key 保存到 ThreadLocalMap 中，但是这里要注意的是它作为一个 key 用的是弱引用，因为没有强引用链，弱引用在 GC 的时候可能会被回收。这样就会在 ThreadLocalMap 中存在一些 key 为 null 的键值对 （Entry）。因为 key 变成 null 了，我们是没法访问这些 Entry 的，但是这些 Entry 本身是 不会被清除的。如果没有手动删除对应 key 就会导致这块内存即不会回收也无法访问，也 就是内存泄漏。

​	使用完 ThreadLocal 之后，记得调用 remove 方法。 在不使用线程池的前提下， 即使不调用 remove 方法，线程的"变量副本"也会被 gc 回收，即不会造成内存泄漏的情况。



# 项目中有没有用到过反射

![image-20231205141926443](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231205141926443.png)



# 缓存的穿透、击穿、雪崩、预热？

https://blog.csdn.net/CYK_byte/article/details/129780182（带图示）

## 1、什么是缓存穿透：

  缓存穿透是指用户请求的数据在缓存和数据库中都不存在，导致用户每次请求该数据都要去数据库中查询一遍。如果有恶意攻击者不断请求系统中不存在的数据，会导致短时间大量请求落在数据库上，造成数据库压力过大，甚至导致数据库承受不住而崩溃。

  缓存穿透就是请求直接从缓存中穿透到数据库了，数据库里也没这个数据，数据库就也一起凉凉了。

  **2、问题分析：**

  缓存穿透的关键在于在Redis中查不到key值，假如有恶意攻击者传进大量的不存在的key，那么大量的请求打在数据库上是很致命的问题，所以在日常开发中要对参数做好校验，一些非法的参数，不可能存在的key就直接返回错误提示。

  **3、解决方法：**

  （1）将无效的key存放进Redis中：

  当出现Redis查不到数据，数据库也查不到数据的情况，我就把这个key保存到Redis中，设置value="null"，并设置其过期时间极短，后面再出现查询这个key的请求的时候，直接返回null，就不需要再查询数据库了。但假如每次传进来的的Key值都是随机的，那存进Redis也没有意义。

  （2）使用bloomfilter布隆过滤器：

  于是我们就在缓存之前再加一个过滤器，将数据库中所有key值都存储在过滤器中，在查询Redis前先去过滤器查询 key 是否存在，如果不存在就直接返回，不让其访问数据库，从而避免了对底层存储系统的查询压力。

- 对key做很多次hash,Int整数值

## 1、什么是缓存击穿：

  缓存击穿是某个热点的key值失效，大并发集中对其进行请求，就会造成大量请求读缓存没读到数据，从而导致高并发访问数据库，引起数据库压力剧增。



  **2、问题分析：**

  关键在于某个热点的key失效了，导致大并发集中打在数据库上。

  **3、解决方案：**

  （1）在缓存失效后，通过互斥锁或者队列来控制读数据写缓存的线程数量，比如某个key只允许一个线程查询数据和写缓存，其他线程等待。这种方式会阻塞其他的线程，此时系统的吞吐量会下降。

  （2）如果业务允许的话，对于热点的key可以设置为永不过期的key 。

  

## 1、什么是缓存雪崩：

  缓存雪崩就是缓存在同一个时刻出现大规模的key失效，那么就会导致大量的请求打在了数据库上面，导致数据库压力巨大，如果在高并发的情况下，可能瞬间就会导致数据库宕机。

  缓存雪崩就是大量key没了，请求像雪崩一样打到数据库 

  **2、问题分析：**

  造成缓存雪崩的关键在于同一时间的大规模的key失效，出现这个问题主要有两种可能：一种是Redis宕机，一种可能就是采用了相同的过期时间。

  **3、解决方案：**

  （1）设置不同的过期时间，避免相同的过期时间导致缓存雪崩，造成大量数据库的访问。 

  如果真的发生了缓存雪崩，还有兜底的措施：

  （2）使用熔断机制。当流量到达一定的值时，就直接给用户返回提示，防止过多的请求打在数据库上。至少能保证一部分用户是可以正常使用，其他用户多刷新几次也能得到结果。

  （3）提高数据库的容灾能力，可以使用分库分表，读写分离的策略。

  （4）为了防止Redis宕机导致缓存雪崩的问题，可以搭建Redis集群，提高Redis的容灾性。

  

## 1、什么是缓存预热：

  缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，让用户直接查询事先被预热的缓存数据。

  **2、问题分析：**

  对于高并发的流量，没有预热，那Redis初始数据为空，就都会访问到数据库中， 对数据库造成流量的压力。

  **3、缓存预热解决方案：**

  （1）数据量不大时，工程启动的时候进行加载缓存动作。

  （2）数据量大时，设置一个定时任务脚本，进行缓存的刷新。

  （3）数据量很大时，优先保证热点数据进行提前加载到缓存。



# 微信小程序的登陆过程

https://zhuanlan.zhihu.com/p/620589067

用户在小程序中点击`登录`按钮，触发登录事件。

小程序调用`wx.login()`方法获取临时`登录凭证`code。

小程序调用 `wx.getUserInfo()`方法获取用户的`基本信息`（头像、昵称等）。

小程序将`code`和用户信息发送给开发者的`后台服务器`。

开发者的后台服务器使用 `AppID`和 `AppSecret` 调用微信接口`获取 session_key 和 openid`。

开发者的后台服务器使用`session_key`对用户信息进行解密，获取用户的真实信息。

开发者的后台服务器将用户的信息存储到数据库中，并生成一个自定义登录态`token`并返回给小程序。

小程序`保存`该自定义登录态`token`，并在以后的`请求`中带上该`token`。

## 登录过程

之前项目中做过一个点单系统，前端用的是小程序，用户必须先登录才能执行点菜，下单等操作，小程序登录时，先调用了wx.login方法,获取到了code (临时登录凭证) ,然后前端也就是小程序端发送请求并携带code发送至服务端，服务端接收到前端发送的code后,要获取到用户的openld,因为openld是用户的唯一标识,所有要获取此属性作为用户的信息存储到数据库,获取openid,通过httpclient向微信接口发送http请求，携带好accessid, accesskey, code等属性(这些属性可以在官方文档查看)，获取到后，是字符串形式的，可以用jSON.parseObject方法他转为对象获取openid, 得到后，先判断是否为空,为空报错,不为空继续执行，然后判断通过openid查询用户表中是否有此用户，无则相当于是新用户，执行插入,将用户的openid插入,并通过usegeneratekey和keyproperty,有则继续执行，之后生成jwt token,将用户的id作为载荷，设置好secret秘钥, 过期时间，生成token, 之后将token, openid, 用户的id返回给小程序端，小程序端将token存储到本地存储中, 以后再登录的时候，前端发送的请求时携带token,服务端通过拦截器，解析token,得到用户id,存入到threadlocal中,以便之后做校验或者设置用户信息,比如提交订单,点选菜品到购物车等

![image-20231202142036650](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231202142036650.png)

`localstorage`改为：`本地存储`

# redis数据如何与mysql数据保持一致

![redis面试题（上）](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/redis%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%8A%EF%BC%89.png)

### (1)先更新数据库，再更新缓存

// 这套方案，大家是普遍反对的。为什么呢？有如下两点原因。
**原因一（线程安全角度）**
同时有请求A和请求B进行更新操作，那么会出现
（1）线程A更新了数据库
（2）线程B更新了数据库
（3）线程B更新了缓存
（4）线程A更新了缓存
这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。
**原因二（业务场景角度）**
有如下两点：
（1）如果你是一个写数据库场景比较多，而读数据场景比较少的业务需求，采用这种方案就会导致，数据压根还没读到，缓存就被频繁的更新，浪费性能。
（2）如果你写入数据库的值，并不是直接写入缓存的，而是要经过一系列复杂的计算再写入缓存。那么，每次写入数据库后，都再次计算写入缓存的值，无疑是浪费性能的。显然，删除缓存更为适合。

接下来讨论的就是争议最大的，先删缓存，再更新数据库。还是先更新数据库，再删缓存的问题。

### (2)先删缓存，再更新数据库

该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:
（1）请求A进行写操作，删除缓存
（2）请求B查询发现缓存不存在
（3）请求B去数据库查询得到旧值
（4）请求B将旧值写入缓存
（5）请求A将新值写入数据库
上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。
那么，**如何解决呢？采用延时双删策略**
伪代码如下

```java
public void write(String key,Object data){
		redis.delKey(key);
	    db.updateData(data);
	    Thread.sleep(1000);
	    redis.delKey(key);
	}
```

转化为中文描述就是
（1）先淘汰缓存
（2）再写数据库（这两步和原来一样）
（3）休眠1秒，再次淘汰缓存
这么做，可以将1秒内所造成的缓存脏数据，再次删除。
**那么，这个1秒怎么确定的，具体该休眠多久呢？**
针对上面的情形，读者应该自行评估自己的项目的读数据业务逻辑的耗时。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。
**如果你用了mysql的读写分离架构怎么办？**
ok，在这种情况下，造成数据不一致的原因如下，还是两个请求，一个请求A进行更新操作，另一个请求B进行查询操作。
（1）请求A进行写操作，删除缓存
（2）请求A将数据写入数据库了，
（3）请求B查询缓存发现，缓存没有值
（4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值
（5）请求B将旧值写入缓存
（6）数据库完成主从同步，从库变为新值
上述情形，就是数据不一致的原因。还是使用双删延时策略。只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms。
**采用这种同步淘汰策略，吞吐量降低怎么办？**
ok，那就将第二次删除作为异步的。自己起一个线程，异步删除。这样，写的请求就不用沉睡一段时间后了，再返回。这么做，加大吞吐量。
**第二次删除,如果删除失败怎么办？**
这是个非常好的问题，因为第二次删除失败，就会出现如下情形。还是有两个请求，一个请求A进行更新操作，另一个请求B进行查询操作，为了方便，假设是单库：
（1）请求A进行写操作，删除缓存
（2）请求B查询发现缓存不存在
（3）请求B去数据库查询得到旧值
（4）请求B将旧值写入缓存
（5）请求A将新值写入数据库
（6）请求A试图去删除请求B写入对缓存值，结果失败了。
ok,这也就是说。如果第二次删除缓存失败，会再次出现缓存和数据库不一致的问题。
**如何解决呢？**
具体解决方案，且看博主对第(3)种更新策略的解析。

### (3)先更新数据库，再删缓存(最常用，重点讲此项)

首先，先说一下。老外提出了一个缓存更新套路，名为[《Cache-Aside pattern》](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)。其中就指出

- **失效**：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
- **命中**：应用程序从cache中取数据，取到后返回。
- **更新**：先把数据存到数据库中，成功后，再让缓存失效。

另外，知名社交网站facebook也在论文[《Scaling Memcache at Facebook》](https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf)中提出，他们用的也是先更新数据库，再删缓存的策略。
**这种情况不存在并发问题么？**
不是的。假设这会有两个请求，一个请求A做查询操作，一个请求B做更新操作，那么会有如下情形产生
（1）缓存刚好失效
（2）请求A查询数据库，得一个旧值
（3）请求B将新值写入数据库
（4）请求B删除缓存
（5）请求A将查到的旧值写入缓存
ok，如果发生上述情况，确实是会发生脏数据。
**然而，发生这种情况的概率又有多少呢？**
发生上述情况有一个先天性条件，就是步骤（3）的写数据库操作比步骤（2）的读数据库操作耗时更短，才有可能使得步骤（4）先于步骤（5）。可是，大家想想，数据库的读操作的速度远快于写操作的（不然做读写分离干嘛，做读写分离的意义就是因为读操作比较快，耗资源少），因此步骤（3）耗时比步骤（2）更短，这一情形很难出现。
**如何解决上述并发问题？**
首先，给缓存设有效时间是一种方案。其次，采用策略（2）里给出的异步延时删除策略，保证读请求完成以后，再进行删除操作。

**还有其他造成不一致的原因么？**
有的，这也是缓存更新策略（2）和缓存更新策略（3）都存在的一个问题，如果删缓存失败了怎么办，那不是会有不一致的情况出现么。比如一个写数据请求，然后写入数据库了，删缓存失败了，这会就出现不一致的情况了。这也是缓存更新策略（2）里留下的最后一个疑问。
**如何解决？**
提供一个保障的重试机制即可，这里给出两套方案。
**方案一**：
如下图所示
![image](https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_update1.png)
流程如下所示
（1）更新数据库数据；
（2）缓存因为种种问题删除失败
（3）将需要删除的key发送至消息队列
（4）自己消费消息，获得需要删除的key
（5）继续重试删除操作，直到成功
然而，该方案有一个缺点，对业务线代码造成大量的侵入。于是有了方案二，在方案二中，启动一个订阅程序去订阅数据库的binlog，获得需要操作的数据。在应用程序中，另起一段程序，获得这个订阅程序传来的信息，进行删除缓存操作。
**方案二**：
![image](https://images.cnblogs.com/cnblogs_com/rjzheng/1202350/o_update2.png)
流程如下图所示：
（1）更新数据库数据
（2）数据库会将操作信息写入binlog日志当中
（3）订阅程序提取出所需要的数据以及key
（4）另起一段非业务代码，获得该信息
（5）尝试删除缓存操作，发现删除失败
（6）将这些信息发送至消息队列
（7）重新从消息队列中获得该数据，重试操作。

**备注说明：**上述的订阅binlog程序在mysql中有现成的中间件叫canal，可以完成订阅binlog日志的功能。至于oracle中，博主目前不知道有没有现成中间件可以使用。另外，重试机制，博主是采用的是消息队列的方式。如果对一致性要求不是很高，直接在程序中另起一个线程，每隔一段时间去重试即可，这些大家可以灵活自由发挥，只是提供一个思路。

![image-20231205141751708](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231205141751708.png)



# @Transactional失效的场景有哪些？

https://zhuanlan.zhihu.com/p/452094574

## 一、失效场景集一：代理不生效

Spring中对注解解析的尿性都是基于代理的，如果目标方法无法被Spring代理到，那么它将无法被Spring进行事务管理。

**Spring生成代理的方式有两种：**

1. 基于接口的JDK动态代理，要求目标代理类需要实现一个接口才能被代理
2. 基于实现目标类子类的CGLIB代理

Spring Boot在2.0之前，目标类如果实现了接口，则使用JDK动态代理方式，否则通过CGLIB子类的方式生成代理。

而在2.0版本之后，如果不在配置文件中显示的指定spring.aop.proxy-tartget-class的值，默认情况下生成代理的方式为CGLIB，如下图



![img](https://pic3.zhimg.com/80/v2-cf758613404531ba95b39a53b9a2f51a_720w.webp)



顺着代理的思路，我们来看看哪些情况会因为代理不生效导致事务管控失败。

**（1）将注解标注在接口方法上**

@Transactional是支持标注在方法与类上的。一旦标注在接口上，对应接口实现类的代理方式如果是CGLIB，将通过生成子类的方式生成目标类的代理，将无法解析到@Transactional，从而事务失效。

这种错误我们还是犯得比较少的，基本上我们都会将注解标注在接口的实现类方法上，官方也不推荐这种。

**（2）被final、static关键字修饰的类或方法**

CGLIB是通过生成目标类子类的方式生成代理类的，被final、static修饰后，无法继承父类与父类的方法。

**（3）类方法内部调用**（重点）

事务的管理是通过代理执行的方式生效的，如果是方法内部调用，将不会走代理逻辑，也就调用不到了。

例如



![img](https://pic2.zhimg.com/80/v2-424cfea19ead6485d64cf72a677f6dfd_720w.webp)



在**createUser**中调用了内部方法**createUser1**，并且**createUser1**方法上设置了事务传播策略为：**REQUIRES_NEW**，但是因为是内部直接调用，createUser1不能不代理处理，无法进行事务管理。在createUser1方法抛出异常后就插入数据失败了。

**但是这种操作在我们业务开发的过程中貌似还挺常见的，怎么样才能保证其成功呢？**

方式1：新建一个Service，将方法迁移过去，有点麻瓜。

方式2：在当前类注入自己，调用createUser1时通过注入的userService调用



![img](https://pic1.zhimg.com/80/v2-2d361440a31390453280cff667fe2900_720w.webp)



方式3：通过AopContext.currentProxy()获取代理对象



![img](https://pic2.zhimg.com/80/v2-2a8fdaa52a05be9e2c99a6b4a7957a29_720w.webp)



道理类似于方式2，就是为了通过代理来访问内部方法

**（4）当前类没有被Spring管理**

这个没什么好说的，都没有被Spring管理成为IOC容器中的一个bean，更别说被事务切面代理到了。

这种Bug看上去比较蠢，但没准真的有人犯错。

## 二、失效场景集二：框架或底层不支持的功能

这类失效场景主要聚焦在框架本身在解析@Transactional时的内部支持。如果使用的场景本身就是框架不支持的，那事务也是无法生效的。

**（1）非public修饰的方法**

我们在标有@Transactional的任意方法上打个断点，在idea内能看到事务切面点如下图所示



![img](https://pic2.zhimg.com/80/v2-3a94d511d4b0aae8ff1ae910c331b441_720w.webp)



点击去这个方法，在开头有这么一个调用



![img](https://pic4.zhimg.com/80/v2-8ff5e3d88cede49907ba5d73068eabf7_720w.webp)



继续进去



![img](https://pic2.zhimg.com/80/v2-5375dc4b59e734d7c672a7807a84aa39_720w.webp)



就能看到这么一句话了



![img](https://pic1.zhimg.com/80/v2-87cfde7f7c8240980df2cf6aefb2c1a4_720w.webp)



不支持非public修饰的方法进行事务管理。

**（2）多线程调用**

跟上面一样的的操作，我们能够逐层进入到TransactionAspectSupport.prepareTransactionInfo方法。

注意看以下这段话



![img](https://pic2.zhimg.com/80/v2-78597e7eaf93ccc38c8eebf061d95c61_720w.webp)



从这里我们得知，事务信息是跟线程绑定的。

因此在多线程环境下，事务的信息都是独立的，将会导致Spring在接管事务上出现差异。

**这个场景我们要尤其注意！**

给大家举个例子

主线程A调用线程B保存Id为1的数据，然后主线程A等待线程B执行完成再通过线程A查询id为1的数据。

这时你会发现在主线程A中无法查询到id为1的数据。因为这两个线程在不同的Spring事务中，本质上会导致它们在Mysql中存在不同的事务中。

Mysql中通过MVCC保证了线程在快照读时只读取小于当前事务号的数据，在线程B显然事务号是大于线程A的，因此查询不到数据。

**（3）数据库本身不支持事务**

比如Mysql的Myisam存储引擎是不支持事务的，只有innodb存储引擎才支持。

这个问题出现的概率极其小，因为Mysql5之后默认情况下是使用innodb存储引擎了。

但如果配置错误或者是历史项目，发现事务怎么配都不生效的时候，记得看看存储引擎本身是否支持事务。

**（4）未开启事务**

这个也是一个比较麻瓜的问题，在Springboot项目中已经不存在了，已经有**DataSourceTransactionManagerAutoConfiguration**默认开启了事务管理。

但是在MVC项目中还需要在applicationContext.xml文件中，手动配置事务相关参数。如果忘了配置，事务肯定是不会生效的。

## 三、失效场景集三：错误使用@Transactional

注意啦注意啦，下面这几种都是高频会出现的Bug！

**（1）错误的传播机制**

Spring支持了7种传播机制，分别为：



![img](https://pic4.zhimg.com/80/v2-b11acc5801948766fa189bdb5159709b_720w.webp)



上面不支持事务的传播机制为：PROPAGATION_SUPPORTS，PROPAGATION_NOT_SUPPORTED，PROPAGATION_NEVER。

如果配置了这三种传播方式的话，在发生异常的时候，事务是不会回滚的。

**（2）rollbackFor属性设置错误**



![img](https://pic1.zhimg.com/80/v2-a8d2d65ab0c5a8a7abdd9e45c280747c_720w.webp)



默认情况下事务仅回滚运行时异常和Error，不回滚受检异常（例如IOException）。

因此如果方法中抛出了IO异常，默认情况下事务也会回滚失败。

我们可以通过指定@Transactional(rollbackFor = Exception.class)的方式进行全异常捕获。

**（3）异常被内部catch**

UserService



![img](https://pic2.zhimg.com/80/v2-5cc793da7414f13c543f91ba0f949785_720w.webp)



UserService1



![img](https://pic4.zhimg.com/80/v2-9ad93064adb58615a9649e1f1c223df3_720w.webp)



如上代码UserService调用了UserService1中的方法，并且捕获了UserService1中抛出的异常。

你将能看到控制台出现这样一个报错：

```text
org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only
复制代码
```

默认情况下标注了@Transactional注解的方法的事务传播机制是**REQUIRED**，它的特性是支持当前事务，也就说加入当前事务。我们在UserService中开始事务，然后再UserService1中抛出异常回滚UserService中的事务，将其标记为只读。

但是在UserSevice中我们捕获了异常，此时UserService上的事务认为正常提交事务。最后在提交时发现事务只读，已经被回滚，则抛出了上述异常。

**因此这里如果需要对特定的异常进行捕获处理，记得再次将异常抛出，让最外层的事务感知到。**

**（4）嵌套事务**

上面是我想同时回滚UserService与UserService1。但是也会有这种场景只想回滚UserService1中报错的数据库操作，不影响主逻辑UserService中的数据库。

有两种方式可以实现上述逻辑：

1.直接在UserService1内的整个方法用try/catch包住

2.在UserService1使用Propagation.REQUIRES_NEW传播机制



![img](https://pic1.zhimg.com/80/v2-d037d0a80f269241cb42ee79e60885c8_720w.webp)

## 四、总结

本文为大家分析@Transactional注解使用过程中失效的12种场景



![img](https://pic4.zhimg.com/80/v2-3d72a39f395f5038c87df846b1e499ab_720w.webp)



最后，@Transactional注解虽香，但是复杂业务逻辑下，为了更好的管理事务与把控业务处理时事务的细粒度，我还是推荐大家使用编程式事务。

# 如何用redis实现分布式锁？

## 实现分布式锁的条件:

首先，为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件：

1. 互斥性。在任意时刻，只有一个客户端能持有锁。
2. 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。
3. 具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁。
4. 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。



![redis实现分布式锁](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231208141757665.png)

## 实现案例:

### 案例一

- 在使用redis同步锁时我用的是setNX，它的特性是当一个线程进入时会设置一个键并返回true，此时相当于上锁，当另一个线程进来时会返回一个false,并阻止它进入，这样会出现一个问题,当服务器因为一些意外问题关闭会导致第一个线程的键一直留在里面并上锁然后后面的线程就会无法进入而堵塞出现死锁的情况.这样就要加上一个过期时间，这样就会解决服务器意外解锁出现死锁的问题.
- 但是当设置过期时间过小时就会出现第二个线程进来时第一个线程仍在处理，当第一个线程处理完时发现他自己的锁已经被释放于是转而去释放第二个线程的锁.这样又会导致第三个线程进入，长时间就会出现刚开始的超卖问题。
  在处理第一个时间问题时我是这样处理的，先创建一个子线程,每5秒确认主线程是否在线，当主线程在线时就将过期时间重置来解决过期时间过短的问题.
  第二个问题就需要给锁加一个唯一id让每个线程根据这个唯一id释放自己的锁这样就不会出现线程乱释放锁的问题.
- 当然redis分布式锁是有一定局限性,如果需要多 机部署就需要用到Redisson来实现了



### 案例二（自定义的）

1. 通过 setNX key value ,当key不存在才设置key

   - 弊端：获取到锁的客户端崩溃，会导致锁永远不释放

   - 解决方案：给锁增加有效期

2. set key value ex time nx（当key不存在时，设置一个time秒后过期的key）
   - 将delete放在finally中：try {set key value ex time nx} catch{} finally{delete key}
   - 注意：释放锁时需要判断，锁是不是自己设置的锁（将当前线程的标识写入value,delete之前先取出value的值判断是不是自己设置的锁）
   - 缺点：无法准确判断合适的过期时间
   - 解决措施：redisson:看门狗

3. redisson:看门狗：延迟锁的有效期
4. 释放锁时：Lua脚本

### 老师案例

setnx  key  value:  获取到锁的客户端崩溃，锁就永远不释放
所以，需要给锁加一个有效期
set key value ex time nx：同时设置有效期和只有不存在才设置，保证一个客户端获取到锁
锁释放，需要注意，只能去释放自己加的锁 不能把别的线程获取的锁给释放掉
value中需要携带加锁的线程信息
加锁的时候，锁的有效期不能太长，也不能太短，取决于的业务的复杂程度
所以，可以加一个后台的定时任务，每隔一段时间去延长锁的有效期，看门狗
1）锁加有效期
2）释放的时候，要判断是不是自己的锁
3）有效期不能太长，也不能太短，使用看门狗，在后台延长锁的有效期

![定时任务面试题](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231207115725084.png)



# 线程池的工作原理**ThreadPoolExecutor**

https://www.cnblogs.com/xfeiyun/p/17228576.html



### 线程池的7个组成部分

![线程池的7个部分](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231214142521987.png)

corePoolSize：指定了线程池中的线程数量。

maximumPoolSize：指定了线程池中的最大线程数量。

keepAliveTime：当前线程池数量超过 corePoolSize 时，多余的空闲线程的存活时间，即多次时间内会被销毁。

unit：keepAliveTime 的单位。

workQueue：任务队列，被提交但尚未被执行的任务。

threadFactory：线程工厂，用于创建线程，一般用默认的即可。

handler：拒绝策略，当任务太多来不及处理，如何拒绝任务。
### 线程池工作过程
1. 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。

2. 当调用 execute() 方法添加一个任务时，线程池会做如下判断：
   1. 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务；
   2. 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列；
   3. 如果这时候队列满了，而且正在运行的线程数量小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务；
   4. 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，则线程池会抛出异常RejectExecutionException。

3. 当一个线程完成任务时，它会从队列中取下一个任务来执行。

4. 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。

   ![img](https://img2023.cnblogs.com/blog/2443180/202303/2443180-20230317230038194-476298880.png)

### 4种拒绝策略
线程池中的线程已经用完了，无法继续为新任务服务，同时，等待队列也已经排满了，再也塞不下新任务了。这时候我们就需要拒绝策略机制合理的处理这个问题。

1. AbortPolicy ： 直接抛出异常，阻止系统正常运行。

2. CallerRunsPolicy ： 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降。

3. DiscardOldestPolicy ： 丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。

4. DiscardPolicy ： 该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案。

以上内置拒绝策略均实现了 RejectedExecutionHandler 接口，若以上策略仍无法满足实际需要，完全可以自己扩展 RejectedExecutionHandler 接口。

# Spring中有哪些常见的设计模式？你的项目中用到了哪些设计模式？

### 用到的设计模式：

工厂/单例/代理/模板

![image-20231217141625737](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231217141625737.png)

![image-20231217141921893](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231217141921893.png)

![image-20231217141816771](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231217141816771.png)
# nasoc注册中心的工作原理
![注册中心原理](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/Pasted%20image%2020231218141419.png)
![nacos特有的功能](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/Pasted%20image%2020231218141355-1702956225351-1.png)

# 项目中用到了SpringCloud的哪些组件？

![image-20231220141425752](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231220141425752.png)

![image-20231220141521540](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231220141521540.png)

![image-20231220141539627](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231220141539627.png)

![image-20231220141627435](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231220141627435.png)





# 什么是请求跨域，如何解决





# volume是什么? volume挂载和本地目录或者文件直接挂载的区别?

![image-20231222141211619](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231222141211619.png)



![image-20231222141229594](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231222141229594.png)

# Spring Bean的生命周期



# 30分钟订单超时未支付如何处理

使用RabbitMQ插件

![image-20231229142208574](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231229142208574.png)

延迟队列插件生成一个新的交换机DelayExchange,添加delayed属性为true。 当我们发送消息到延迟交换机delayExchange时，会判断消息是否具有x-delay属性,如果有x-delay属性,说明是延迟消息，那么这些消息不会进行投递，就会存到Mnesia table (一个分布式数据库)中,并读取x-delay属性值作为延迟时间，在x- delay延迟时间30分钟到期后，就会重新投递消息到指定队列中。

![image-20231229142658144](https://kaidenpic.oss-cn-qingdao.aliyuncs.com/image-20231229142658144.png)
